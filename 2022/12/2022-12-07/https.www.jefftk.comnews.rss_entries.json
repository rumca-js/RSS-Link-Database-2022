[{"source": "https://www.jefftk.com/news.rss", "title": "Machine Learning Consent", "description": "<p><span>\n\nFor years, researchers have trained machine learning systems on\nwhatever data they could find. People mostly haven't cared about this\nor paid attention, I think because the systems hadn't been very good.\nRecently, however, some very impressive systems have come out,\nincluding ones that </span>\n\n<a href=\"https://en.wikipedia.org/wiki/ChatGPT\">answer questions</a>, \n\n<a href=\"https://en.wikipedia.org/wiki/GitHub_Copilot\">complete\ncode</a>, and \n\n<a href=\"https://en.wikipedia.org/wiki/DALL-E\">generate</a> \n\n<a href=\"https://en.wikipedia.org/wiki/Midjourney\">images</a> \n\n<a href=\"https://en.wikipedia.org/wiki/Stable_Diffusion\">from prompts</a>.\n\n\n\n<p>\n\nBecause these are so capable a lot more people are paying attention\nnow, and there are big questions around whether it's ok that these\nsystems were trained this way.  Code that I uploaded to GitHub and the\nwriting that I've put into this blog went into training these models:\nI didn't give permission for this kind of use, and no one asked me if\nit was ok.  Doesn't this violate my copyrights?\n\n</p>\n\n<p>\n\n\nThe machine learning community has generally assumed that training\nmodels on some input and using it to generate new output is legal, as\nlong as the output is sufficiently different from the input.  This\nrelies on the doctrine of \"<a href=\"https://en.wikipedia.org/wiki/Fair_use\">fair use</a>\", which\ndoes not require any sort of permission from the original author as\nlong as it is sufficiently \"transformative\".  For example, if I took a\nbook and replaced every instance of the main characters name with my\nown I doubt any court would consider that sufficiently transformative,\nand so my book would be considered a \"derivative work\" of the original\nbook.  On the other hand, if I took the words in the book and\npainstakingly reordered them to tell a completely unrelated story,\nthere's a sense in which my book was \"derived\" from the original one\nbut I think it would pretty clearly be transformative enough that I\nwouldn't need any permission from the copyright holder.\n\n</p>\n\n<p>\n\nThese models can be used to create things that are clearly derivative\nworks of their input. For example, people <a href=\"https://mobile.twitter.com/mitsuhiko/status/1410886329924194309\">very\nquickly realized</a> that Copilot would complete the code for <a href=\"https://www.beyond3d.com/content/articles/15/\">Greg Walsh's</a>\n<a href=\"https://en.wikipedia.org/wiki/Fast_inverse_square_root\">fast\ninverse square root</a> implementation verbatim, and if you ask any of\nthe image generators for the <a href=\"https://en.wikipedia.org/wiki/Mona_Lisa\">Mona Lisa</a> or <a href=\"https://en.wikipedia.org/wiki/The_Starry_Night\">Starry Night</a>\nyou'll get something close enough to the original that it's clearly a\nknock-off.  This is a major issue with current AI systems, but it's\nalso a relatively solvable one. It's already possible to slowly check\nthat the output doesn't excessively resemble any input, and I think\nit's likely they'll soon figure out how to do that efficiently.  On the\nother hand, all of the examples of this I've seen (and I just did some\n<a href=\"https://www.reddit.com/r/ArtistLounge/comments/xkbd1l/examples_of_ai_plagiarizing_existing_art/\">looking</a>)\nhave been people trying to elicit plagiarism.\n\n</p>\n\n<p>\n\nThe normal use case is much more interesting, and more controversial.\nWhile the transformative fair use justification I described above is\nwidely assumed within the machine learning community as far as I can\ntell it hasn't been tested in court. There is currently a <a href=\"https://www.reddit.com/r/ArtistLounge/comments/xkbd1l/examples_of_ai_plagiarizing_existing_art/\">large\nclass action lawsuit</a> over Copilot, and it's possible this kind of\nusage will turn out not qualify.  Speculating, I think it's pretty\nunlikely that the suit will succeed, but I've created a <a href=\"https://manifold.markets/JeffKaufman/will-the-github-copilot-litigation\">prediction\nmarket</a> on it to gather information:\n\n</p>\n\n<p>\n\n\n\n</p>\n\n<p>\n\nAside from the legal question, however, there is also a moral or\nsocial question: is it ok to train a model on someone's work without\ntheir permission? What if this means that they and others in their\nprofession are no longer able to earn a living?\n\n</p>\n\n<p>\n\nOn the second question, you could imagine someone creating\na model where they used only data that was either in the public domain\nor which they'd purchased appropriate licenses for.  While that's\ngreat for the particular people who agree and get paid, a much larger\nnumber would still be out of work without compensation.  I do think\nthere's potentially quite a bad situation, where as these systems get\nbetter more and more people are unable to add much over an automated\nsystem, and we get massive <a href=\"https://en.wikipedia.org/wiki/Technological_unemployment\">technological\nunemployment</a>.  Now, historically worries here proved unfounded, and\ntechnology has consistently been much more of a human complement than\nhuman substitute.  As the saying goes, however, that was also the case\nfor horses until it wasn't.  I think a <a href=\"https://en.wikipedia.org/wiki/Universal_basic_income\">Universal\nBasic Income</a> is probably the best approach here.\n\n</p>\n\n<p>\n\nOn the first question, learning from other people's work without their\nconsent is something humans do all the time.  You can't draw too\nheavily on any one thing you've seen without following a complex set\nof rules about permission and acknowledgement, but human creative work\ngenerally involves large amounts of borrowing.  These machine learning\nsystems are not humans, but they are fundamentally doing a pretty\nsimilar thing when they learn from examples, and I don't see a strong\nreason to treat their work differently here.  Because these systems\ndon't currently understand how much borrowing is ok we do need to\napply our own judgment to avoid technologically-facilitated\nplagiarism, but the normal case of creating something relatively\noriginal that pulls from a wide range of prior work is fine for us to\ndo with our brains and should be equally ok for us to do with our\ntools.\n\n  </p>\n\n<p><i>Comment via: <a href=\"https://www.facebook.com/jefftk/posts/pfbid0zVuJxxMMSMGbSU8hA2hDG88kffHzTJpSY73q6CBNmZaaHwa5CskM22iHmkDtB859l\">facebook</a>, <a href=\"https://lesswrong.com/posts/dqQGrPRYamrtLvdLz\">lesswrong</a>, <a href=\"https://mastodon.mit.edu/@jefftk/109475991749825173\">mastodon</a></i></p>", "link": "https://www.jefftk.com/p/machine-learning-consent", "date_published": "2022-12-07 08:00:00+00:00", "persistent": false, "user": null, "language": "en-US"}]